{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Data Engineering Capstone Project\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in /opt/conda/lib/python3.6/site-packages (0.15.1)\n",
      "Requirement already satisfied: numpy>=1.14 in /opt/conda/lib/python3.6/site-packages (from pyarrow) (1.18.0)\n",
      "Requirement already satisfied: six>=1.0.0 in /opt/conda/lib/python3.6/site-packages (from pyarrow) (1.11.0)\n",
      "Requirement already satisfied: fastparquet in /opt/conda/lib/python3.6/site-packages (0.3.2)\n",
      "Requirement already satisfied: numba>=0.28 in /opt/conda/lib/python3.6/site-packages (from fastparquet) (0.35.0)\n",
      "Requirement already satisfied: thrift>=0.11.0 in /opt/conda/lib/python3.6/site-packages (from fastparquet) (0.13.0)\n",
      "Requirement already satisfied: numpy>=1.11 in /opt/conda/lib/python3.6/site-packages (from fastparquet) (1.18.0)\n",
      "Requirement already satisfied: pandas>=0.19 in /opt/conda/lib/python3.6/site-packages (from fastparquet) (0.23.3)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from fastparquet) (1.11.0)\n",
      "Requirement already satisfied: llvmlite in /opt/conda/lib/python3.6/site-packages (from numba>=0.28->fastparquet) (0.20.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /opt/conda/lib/python3.6/site-packages (from pandas>=0.19->fastparquet) (2.6.1)\n",
      "Requirement already satisfied: pytz>=2011k in /opt/conda/lib/python3.6/site-packages (from pandas>=0.19->fastparquet) (2017.3)\n"
     ]
    }
   ],
   "source": [
    "# Do all imports and installs here\n",
    "!pip install pyarrow\n",
    "!pip install fastparquet\n",
    "from datetime import timedelta, datetime\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import configparser\n",
    "from botocore.exceptions import ClientError\n",
    "#import pyarrow\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "from pyspark.sql.functions import col,udf,date_format\n",
    "from pyspark.sql import SparkSession\n",
    "import re\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 1: Scope the Project and Gather Data\n",
    "\n",
    "### Project plan and end goal\n",
    "\n",
    "The scope of the project is to gain insights on immigration to the United States based on source data from few different sources. The source data will be linked together through a relational data model and go through a ETL process and reside into a AWS Redshift analytics database from where it can be queried for analytics and insights can be produced in the form of visual dashboards. \n",
    "\n",
    "In addition to that some of the data may need to be accessed more frequently than others and require to be queried by management for quick high level information therefore a copy of that data from AWS Redshift can then be put together into one table in the form of a curated dataset which will be stored and can be queried using AWS Athena which is much cheaper and more suitable to this use case. This data can then also be quickly transformed to visual dashboards in a very user friendly manner using AWS Quicksight which is already integrated to AWS Athena.\n",
    "\n",
    "### Data sources\n",
    "\n",
    "\n",
    "### Tools\n",
    "\n",
    "The tools that were used in this project are as follows:\n",
    "\n",
    "* Python\n",
    "* Spark\n",
    "* AWS S3\n",
    "* AWS Redshift\n",
    "* Apache Airflow\n",
    "\n",
    "### Describe and Gather Data \n",
    "\n",
    "* I94 Immigration Data: This data comes from the US National Tourism and Trade Office. A data dictionary is included in the workspace. [This](https://travel.trade.gov/research/reports/i94/historical/2016.html) is where the data comes from. This dataset contains personal metadata information about immirgrants such as their gender, birth year etc. and information about their immigration to the Untied States such as (visatype, which airline they came to US with and which airport they landed in etc.). This is the main dataset and there is a file for each month of the year of 2016 available in the Directory `../../data/18-83510-I94-Data-2016/`. Combined, the 12 datasets have got more than 40 million rows and 28 columns. For most of the work I used only the dataset for the month of April of 2016 which has more than three million records.\n",
    "\n",
    "* World Temperature Data: [This](https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data) dataset came from Kaggle. This dataset contains information about temperature across different cites of different countries in the world.\n",
    "* U.S. City Demographic Data: [This](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/) data comes from OpenSoft. You can read more about it here. This dataset contains infromation about demographics of different cities within different states of US.\n",
    "* Airport Code Table: [This](https://datahub.io/core/airport-codes#data) is a simple table of airport codes and corresponding cities. It comes from here. This dataset contains information about different IATA airport codes from around the world.\n",
    "* I94_SAS_Labels_Descriptions - This is a SAS file that contains some more details about the Immigration data which can be used to gain useful insights. For example, with these label descriptions one can gain information about the country of birth and residence of the immigrant.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()\n",
    "\n",
    "# Read in the data here\n",
    "\n",
    "# Read immigration data in from the S3 bucket and load into a Spark dataframe\n",
    "immigrationdf_spark =spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')\n",
    "sas_labels = pd.read_csv('SAS_Labels.csv')\n",
    "\n",
    "# Read the data for World temperature data and load into a Spark dataframe\n",
    "world_temperature_file_path = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "world_temperature_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(world_temperature_file_path)\n",
    "\n",
    "# Read the data for US city demographics and load into a Spark dataframe\n",
    "us_cities_demographicsdf = spark.read.format(\"csv\").option(\"sep\", \";\").option(\"header\", \"true\").load(\"us-cities-demographics.csv\")\n",
    "\n",
    "# Read the data for airport codes into a Spark dataframe\n",
    "airport = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"airport-codes_csv.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def preprocess_sas_labels(sas_labels):\n",
    "    \"\"\"\n",
    "    This function takes in the pandas dataframe loaded with sas label descriptions giving us information about how a country code relates to an actual country\n",
    "    and state codes relate to an actual US State. The function then performs various cleaning activites on the data and writes it back into two .csv files, \n",
    "    one for country and one for state.\n",
    "    \n",
    "    :param :sas_labels: Dataframe containing information about country codes and state codes with corresponding meaning of the codes\n",
    "    :return :None\n",
    "    \"\"\"\n",
    "    Countries = sas_labels['Country'].str.split(pat = \"=\", expand=True)\n",
    "    States = sas_labels['State'].str.split(pat = \"=\", expand=True)\n",
    "    States.columns = ['StateCode', 'StateName']\n",
    "    Countries.columns = ['CountryCode','CountryName']\n",
    "    Countries.CountryCode = Countries.CountryCode.str.strip()\n",
    "    States.StateCode = States.StateCode.str.strip()\n",
    "    for i in range(0,States.StateCode.shape[0]):\n",
    "        try:\n",
    "            States['StateCode'][i] = States['StateCode'][i][1:-1]    \n",
    "        except:\n",
    "            continue\n",
    "    Countries.to_csv('countries.csv')\n",
    "    States.to_csv('states.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "preprocess_sas_labels(sas_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Load in the cleaned sas_labels files\n",
    "df_countries = spark.read.format(\"csv\").option(\"header\", \"true\").load('countries.csv')\n",
    "df_states = spark.read.format(\"csv\").option(\"header\", \"true\").load('states.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# SAS datetime conversion\n",
    "def etl_immigration_data(immigrationdf_spark, df_countries, df_states):\n",
    "    \"\"\"\n",
    "    This is the main function that extracts, transforms and cleans the immigration data using Spark.\n",
    "    \n",
    "    :param  :immigrationdf_spark: This is the dataframe containing information about the immigration dataset\n",
    "    :param  :df_countries: This is the dataframe containing information about countries and their codes\n",
    "    :param  :df_states: This is the dataframe containing information about US states and their codes\n",
    "    :return :immigrationdf_spark: This is the updated immigrationdf_spark dataframe\n",
    "    \"\"\"\n",
    "    convert_sas_udf = udf(lambda x: x if x is None else (timedelta(days=x) + datetime(1960, 1, 1)).strftime(\"%Y-%m-%d\"))\n",
    "    cols = ['cicid','i94yr','i94mon','i94mode','i94bir','i94visa','i94cit','i94res']\n",
    "    dict1 = {'CountryCode':'i94cit','CountryCode':'i94res'}\n",
    "    dict2 = {'StateCode':'i94addr'}\n",
    "    \n",
    "    def convert_sas_date(df, cols):\n",
    "        \"\"\"\n",
    "        Convert dates in the SAS datatype to a date in a string format YYYY-MM-DD\n",
    "\n",
    "        :param :df: Spark dataframe to be processed.             \n",
    "        :param :cols: Column in the SAS date format to be convert    \n",
    "        :return :df: Updated Spark dataframe with string dates\n",
    "        \"\"\"\n",
    "        df = df.withColumn(cols, convert_sas_udf(df[cols]))\n",
    "        return df\n",
    "    # Convert the below columns to Integer\n",
    "\n",
    "    def data_type_conversion(df,cols):\n",
    "        \"\"\"\n",
    "        This function is used to convert all the floating point values to integers and strings as appropriate.\n",
    "\n",
    "        :param :df: This is the dataframe containing immigration information\n",
    "        :param :cols: This is the list of columns whose data type needs to be converted\n",
    "        :return :df: returns the input dataframe with updated datatypes in specific columns\n",
    "        \"\"\"\n",
    "        for col in cols:\n",
    "            df = df.withColumn(col, df[col].cast(IntegerType()))\n",
    "        # Convert the below columns to string\n",
    "        df = df.withColumn('i94cit', df['i94cit'].cast(StringType()))\n",
    "        df = df.withColumn('i94res', df['i94res'].cast(StringType()))\n",
    "        return df\n",
    "    \n",
    "    def join_sas_label_data(df1,df2,dict):\n",
    "        \"\"\"\n",
    "        This function loads in data from SAS labels file and compares the labels against codes in us_immigration_df to provide \n",
    "        more meaningful values.\n",
    "\n",
    "        :param :df1: us_immmigration_df: This is the dataframe containing immigration information  \n",
    "        :param :df2: This is a SAS file containing the corresponding values for each of the codes for country, states, port etc.\n",
    "        :param :dict: A dictionary containing key value mappings of columns in df1 and df2\n",
    "        :return :df1: returns the updated input dataframe \n",
    "        \"\"\"\n",
    "    \n",
    "        for key,value in dict.items():\n",
    "            split_list = re.findall('[A-Z][^A-Z]*', key)\n",
    "            df1 = df1.join(df2, df2[key] == df1[value], \"left\")\n",
    "            columns_to_drop = ['_c0', key, value]\n",
    "            df1 = df1.drop(*columns_to_drop)\n",
    "            df1 = df1.withColumnRenamed(split_list[0] + \"Name\",value)        \n",
    "        return df1\n",
    "        \n",
    "    immigrationdf_spark = data_type_conversion(immigrationdf_spark,cols)\n",
    "    immigrationdf_spark = join_sas_label_data(immigrationdf_spark,df_countries,dict1)\n",
    "    immigrationdf_spark = join_sas_label_data(immigrationdf_spark,df_states,dict2)\n",
    "    immigrationdf_spark = convert_sas_date(immigrationdf_spark,'arrdate')\n",
    "    immigrationdf_spark = convert_sas_date(immigrationdf_spark,'depdate')\n",
    "    unwanted_columns = ['visapost', 'occup', 'entdepu', 'insnum','dtadfile','biryear','dtaddto','admnum','count' ]\n",
    "    immigrationdf_spark =  immigrationdf_spark.drop(*unwanted_columns)\n",
    "    return immigrationdf_spark\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "immigrationdf_spark = etl_immigration_data(immigrationdf_spark, df_countries, df_states)\n",
    "immigrationdf_spark.write.mode(\"Overwrite\").parquet('immigration_data.parquet')\n",
    "#immigrationdf_spark.write.parquet('immigration_data.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "* All the columns that had very little information such as the `visapost` column in immigration dataset and columns that had mostly null values such as the `occup` column, `entdepu` column were dropped as these columns provide no value when looking at bulk of data and only complicates the data model if kept.\n",
    "\n",
    "* All columns that contain values but the information in them do not add any value for this project such as `elevation_ft` column in the airport dataset has also been dropped.\n",
    "\n",
    "* The `dt` and temperature columns from the world temperature dataset were dropped as the temperature was only from 1743-2013 whereas the main dataset which is the immirgration dataset only contained information for the year 2016. However, the country, city and location co-ordinates were still kept as in future it maybe possible to link that to another dataset that contains useful information such as weather or map data for different cities or location co-ordinates.\n",
    "\n",
    "* The airport dataset was completely left out because firstly, there were no common fields between the immigration dataset and the airport dataset, atleast not with relaible consistency and even if we could join these two tables we would not be gaining any additional information. Secondly, it could be interesting to find out information about weather data near the airports by joining the world temperature dataset with the airport dataset as they both have a common field about city but considering that the the weather data is only valid till 2013 and the main goal of this project is to gain useful insight about immigration\n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "\n",
    "* All the SAS dates were converted to a valid date so that the immigrants real date of arrival and departure can be seen for ease of analytics.\n",
    "* The `i94cit`,`i94res` and `i94addr` columns contained codes whose meanings were present in the SAS Labels Description file. Contents of this file was copied across to a `.csv` file which was then loaded into Pandas dataframe. A number of cleaning steps were performed here and a clean dataframe was written back to `.csv` files which was then loaded into Spark. This was done because the cleaning steps required were quite tedious and low level and on top of that the SAS labels file was of a very small size so using Spark to do the cleaning in this particular case would be inefficient.\n",
    "* The codes and their meanings were initially loaded from SAS label csv file into the dataframe as one column and looked like `209 = Qatar`, therefore, they were separated into two columns.\n",
    "* The codes also had double quotes around the single quotes which represented that they were string. The double quotes were removed otherwise it would not match against the code in the immigration dataset.\n",
    "* The codes in the immigration dataset were floats so they needed to be converted to string.\n",
    "* All column names that had white spaces and capitilization were renamed to ensure robustness when data is getting copied into the tables in redshift\n",
    "* Joins were performed in the Spark dataframes to link the code in the immigration dataset with the actual meaning, for example, the column `i94cit` now contains values such as `Qatar`, `Germany` instead of 209.0 or 148.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Select only the valuable and useful columns from the world temperature dataset\n",
    "unwanted_cols = ['dt', 'AverageTemperature', 'AverageTemperatureUncertainty']\n",
    "world_temperature_df = world_temperature_df.drop(*unwanted_cols)\n",
    "for col in world_temperature_df.columns:\n",
    "    world_temperature_df = world_temperature_df.withColumnRenamed(col, col.lower())\n",
    "world_temperature_df.write.mode(\"Overwrite\").parquet('world_temperature_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Rename the column names as white spaces and capitilization of column names may break the code when inserting values into Redshift tables\n",
    "old_columns = ['City','State','Median Age','Male Population','Female Population','Total Population','Foreign-born', 'Average Household Size','State Code','Race','Count']\n",
    "new_columns = ['city','state','medianage','malepopulation','femalepopulation','totalpopulation','foreignborn','averagehouseholdsize','statecode','race', 'countofrace']\n",
    "unwanted_columns = ['Number of Veterans']\n",
    "us_cities_demographicsdf = us_cities_demographicsdf.drop(*unwanted_columns)\n",
    "for i in range(0,len(old_columns)):        \n",
    "    us_cities_demographicsdf = us_cities_demographicsdf.withColumnRenamed(old_columns[i], new_columns[i])\n",
    "us_cities_demographicsdf.write.mode(\"Overwrite\").parquet('us_demographic_data.parquet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "\n",
    "<img src=\"https://github.com/arahman5/udacity-capstone-project/blob/master/model_capstone.PNG\">\n",
    "\n",
    "\n",
    "The above represents the entity relationship diagrom for this project and below are the reasons for choosing a relational data model with star schema\n",
    "\n",
    "* The data is structured and consistent as we know how the data is stored inside the datasets and what are their data types. \n",
    "* Easier to change to business requirements and flexibility in queries as the analytics team would want to perform ad-hoc queries to get a better understanding of the data\n",
    "* The analytics team would want to be able to do aggregations and analytics on the data.\n",
    "* The ability to do JOINs would be very useful here due to the way data is coming from different sources\n",
    "* Star schema supports denormalization of the data, which would be quite useful in this analytics case as this will allow the analytics team to execute simple queries and fast aggregations on the data. \n",
    "* Star schema supports one to one mapping, which is easy to implement and works very well in this case due to the small number of columns in the tables. \n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "\n",
    "* The input data from S3 bucket first goes through ETL Processing in Python using Apache Spark and data is then stored as `.parquet files` in S3.\n",
    "* Data from Parquet files are copied across to tables on AWS Redshift for analytics team to perform analytics.\n",
    "* Apache Airflow is used to automatically schedule and monitor the data loading into the Redshift database.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "\n",
    "Apache Airflow is used to connect to the redshift cluster after the below cell is executed and airflow then starts the process of loading data into the tables in the database within the cluster from the `.parquet` files within the S3 bucket. Launch Airflow Scheduler and Airflow webserver from the terminal and run the DAG `capstone_project`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Set up and Configure redshift cluster\n",
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('dwh.cfg'))\n",
    "\n",
    "KEY                    = config.get('AWS','KEY')\n",
    "SECRET                 = config.get('AWS','SECRET')\n",
    "\n",
    "DWH_CLUSTER_TYPE       = config.get(\"DWH\",\"DWH_CLUSTER_TYPE\")\n",
    "DWH_NUM_NODES          = config.get(\"DWH\",\"DWH_NUM_NODES\")\n",
    "DWH_NODE_TYPE          = config.get(\"DWH\",\"DWH_NODE_TYPE\")\n",
    "\n",
    "DWH_CLUSTER_IDENTIFIER = config.get(\"DWH\",\"DWH_CLUSTER_IDENTIFIER\")\n",
    "DWH_DB                 = config.get(\"DWH\",\"DWH_DB\")\n",
    "DWH_DB_USER            = config.get(\"DWH\",\"DWH_DB_USER\")\n",
    "DWH_DB_PASSWORD        = config.get(\"DWH\",\"DWH_DB_PASSWORD\")\n",
    "DWH_PORT               = config.get(\"DWH\",\"DWH_PORT\")\n",
    "\n",
    "DWH_IAM_ROLE_NAME      = config.get(\"DWH\", \"DWH_IAM_ROLE_NAME\")\n",
    "\n",
    "(DWH_DB_USER, DWH_DB_PASSWORD, DWH_DB)\n",
    "\n",
    "pd.DataFrame({\"Param\":\n",
    "                  [\"DWH_CLUSTER_TYPE\", \"DWH_NUM_NODES\", \"DWH_NODE_TYPE\", \"DWH_CLUSTER_IDENTIFIER\", \"DWH_DB\", \"DWH_DB_USER\", \"DWH_DB_PASSWORD\", \"DWH_PORT\", \"DWH_IAM_ROLE_NAME\"],\n",
    "              \"Value\":\n",
    "                  [DWH_CLUSTER_TYPE, DWH_NUM_NODES, DWH_NODE_TYPE, DWH_CLUSTER_IDENTIFIER, DWH_DB, DWH_DB_USER, DWH_DB_PASSWORD, DWH_PORT, DWH_IAM_ROLE_NAME]\n",
    "             })\n",
    "\n",
    "ec2 = boto3.resource('ec2',\n",
    "                       region_name=\"us-west-2\",\n",
    "                       aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET\n",
    "                    )\n",
    "\n",
    "s3 = boto3.resource('s3',\n",
    "                       region_name=\"us-west-2\",\n",
    "                       aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET\n",
    "                   )\n",
    "\n",
    "iam = boto3.client('iam',aws_access_key_id=KEY,\n",
    "                     aws_secret_access_key=SECRET,\n",
    "                     region_name='us-west-2'\n",
    "                  )\n",
    "\n",
    "redshift = boto3.client('redshift',\n",
    "                       region_name=\"us-west-2\",\n",
    "                       aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET\n",
    "                       )\n",
    "\n",
    "roleArn = iam.get_role(RoleName=DWH_IAM_ROLE_NAME)['Role']['Arn']\n",
    "\n",
    "try:\n",
    "    response = redshift.create_cluster(        \n",
    "        #HW\n",
    "        ClusterType=DWH_CLUSTER_TYPE,\n",
    "        NodeType=DWH_NODE_TYPE,\n",
    "        NumberOfNodes=int(DWH_NUM_NODES),\n",
    "\n",
    "        #Identifiers & Credentials\n",
    "        DBName=DWH_DB,\n",
    "        ClusterIdentifier=DWH_CLUSTER_IDENTIFIER,\n",
    "        MasterUsername=DWH_DB_USER,\n",
    "        MasterUserPassword=DWH_DB_PASSWORD,\n",
    "        \n",
    "        #Roles (for s3 access)\n",
    "        IamRoles=[roleArn]  \n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "def prettyRedshiftProps(props):\n",
    "    pd.set_option('display.max_colwidth', -1)\n",
    "    keysToShow = [\"ClusterIdentifier\", \"NodeType\", \"ClusterStatus\", \"MasterUsername\", \"DBName\", \"Endpoint\", \"NumberOfNodes\", 'VpcId']\n",
    "    x = [(k, v) for k,v in props.items() if k in keysToShow]\n",
    "    return pd.DataFrame(data=x, columns=[\"Key\", \"Value\"])\n",
    "\n",
    "myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "prettyRedshiftProps(myClusterProps)\n",
    "\n",
    "DWH_ENDPOINT = myClusterProps['Endpoint']['Address']\n",
    "DWH_ROLE_ARN = myClusterProps['IamRoles'][0]['IamRoleArn']\n",
    "\n",
    "try:\n",
    "    vpc = ec2.Vpc(id=myClusterProps['VpcId'])\n",
    "    defaultSg = list(vpc.security_groups.all())[0]\n",
    "    \n",
    "    defaultSg.authorize_ingress(\n",
    "        GroupName=defaultSg.group_name,\n",
    "        CidrIp='0.0.0.0/0',\n",
    "        IpProtocol='TCP',\n",
    "        FromPort=int(DWH_PORT),\n",
    "        ToPort=int(DWH_PORT)\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# Delete the cluster\n",
    "redshift.delete_cluster( ClusterIdentifier=DWH_CLUSTER_IDENTIFIER,  SkipFinalClusterSnapshot=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "\n",
    "* All the tables have a Primary key constraint and a not null constraint on any foreign keys in the table.\n",
    "* Count check is peformed to check whether there were actually any data loaded into the table.\n",
    "* Count check is performed to count the no. of rows inserted into the table.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "\n",
    "| Column Name | Description | Source |\n",
    "| :--- | :--- | :--- |\n",
    "| CICID | ID that uniquely identify one record in the dataset | Immigration dataset\n",
    "| I94YR | 4 digit year | Immigration dataset\n",
    "| I94MON | Numeric month | Immigration dataset\n",
    "| I94CIT | 3 digit code of source city for immigration (Born country) | Immigration dataset\n",
    "| I94RES | 3 digit code of source country for immigration (Residence country) | Immigration dataset\n",
    "| I94PORT | Port addmitted through | Immigration dataset\n",
    "| ARRDATE | Arrival date in the USA | Immigration dataset\n",
    "| I94MODE | Mode of transportation | Immigration dataset\n",
    "| I94ADDR | State of arrival | Immigration dataset\n",
    "| DEPDATE | Departure date | Immigration dataset\n",
    "| I94BIR | Age of Respondent in Years | Immigration dataset\n",
    "| I94VISA | Visa codes  | Immigration dataset\n",
    "| ENTDEPA | Arrival Flag. Whether admitted or paroled into the US | Immigration dataset\n",
    "| ENTDEPD | Departure Flag. Whether departed, lost visa, or deceased | Immigration dataset\n",
    "| MATFLAG | Match flag | Immigration dataset\n",
    "| GENDER | Gender | Immigration dataset\n",
    "| AIRLINE | Airline used to arrive in U.S. | Immigration dataset\n",
    "| FLTNO | Flight number of Airline used to arrive in U.S. | Immigration dataset\n",
    "| VISATYPE | Class of admission legally admitting the non-immigrant to temporarily stay in U.S. | Immigration dataset    \n",
    "| City | City Name | World temperature dataset\n",
    "| Country | Country Name | World temperature dataset\n",
    "| Latitude | Latitude | World temperature dataset\n",
    "| Longitude | Longitude | World temperature dataset\n",
    "| State | US state of the city | US Demographic dataset\n",
    "| Median Age | The median of the age of the population | US Demographic dataset\n",
    "| Male Population | Number of the male population | US Demographic dataset\n",
    "| Female Population | Number of the female population | US Demographic dataset\n",
    "| Total Population | Number of the total population | US Demographic dataset\n",
    "| Foreign-born | Number of residents of the city that were not born in the city | US Demographic dataset\n",
    "| Average Household Size | Average size of the houses in the city | US Demographic dataset\n",
    "| State Code | Code of the state of the city | US Demographic dataset\n",
    "| Race | Race class | US Demographic dataset\n",
    "| Count | Number of individual of each race | US Demographic dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "\n",
    "### Choice of tools\n",
    "* Spark - Spark was used for the ETL Process as that's the most process intensive part of the pipeline and Spark is made for dealing with that. Using basic python data stack libraries such as Pandas it would take ~5 minutes to just read in only one of the immigration dataset since they are really huge and takes similar time to write the data out after cleaning it. Whereas, using spark it takes only 30 seconds to perform read and write each separately. In addition to that, the PySpark library is well developed and can be used to perform pretty much everything the usual python data stack libraries perform when it comes to extracting and cleaning the data, however, it does require more effort from the data engineers perspective. But the effort is well worth it as it provides a more faster, robust and scalable data pipeline solution. \n",
    "\n",
    "* S3 was chosen as the storage area for the files because it provides a relatively cheap data storage solution and it is very easy to use as there are python APIs available such as the library `boto3` also can be accessed easily via the AWS CLI. In addition to that, S3 has high availability as there will always be more storage area available immediately just we need to pay for more resource. AWS Identity and access management makes it very secure as well.\n",
    "\n",
    "* Redshift was chosen as the main analytics database because the data is already stored in the cloud in AWS. In addition to that, Redshift provides a massively parallel, column-oriented data warehouse which works perfectly for a columnar data format like `.parquet`. All the mainstream analytical tools such as Tableau, Rapidminer, Python etc. is able to load data from the Redshift database.\n",
    "\n",
    "* Apache Airflow was chosen as the main tool for scheduling and monitoring the loading of the data into Redshift as it has a lot of built in functions which are very useful, such as ability to set up task dependency to ensure everything is automated, ability to connect to AWS services (S3 and redshift), ability to notify in the case a task fails and also able to automatically retry the task without any user intervention.\n",
    "\n",
    "### Updating the data\n",
    "\n",
    "The data should be updated based on the frequency of its ingestion. Since immigration dataset is the main dataset in this project, the update frequency of the database should be based on how often new data is available for the Immigration dataset. In this case, it should be a monthly update as new files are generated every month.\n",
    "\n",
    "### Data increased by 100x\n",
    "\n",
    "If the data was increased by 100x then this wouldn't be a problem as AWS S3 is highly scalable and scale upto match whatever the data storage needs are. Apache Spark can run on AWS EMR so it can be scaled easily as required to deal with the ETL pipeline. Amazon Redshift is a data warehouse that can expand to exabyte-scale so loading the data will not be a problem either. \n",
    "\n",
    "### Dashboard update at 7am\n",
    "\n",
    "The airflow schedule can be tuned to run the DAG for this project everyday so that all required data is loaded into Redshift the night before and a live Dashboard connected to the Database via Tableau or AWS QuickSight will then be updated by 7am every day.\n",
    "\n",
    "### Access by 100+ people \n",
    "AWS Redshift is a highly scalable and available database with a very fast query engine that was designed specifically for Online Analytical Processing (OLAP), therefore, this will not be a problem at all."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
